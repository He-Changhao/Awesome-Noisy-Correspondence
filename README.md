# Noisy-Correspondence Summary (Updating)

# False Positive
 
## Image-Text Matching


#### 2024


- `[2024 AAAI]` **Negative Pre-aware for Noisy Cross-modal Matching**  
*Xu Zhang, Hao Li, Mang Ye*  
[[paper]](https://arxiv.org/pdf/2312.05777.pdf)
[[code]](https://github.com/ZhangXu0963/NPC)

#### 2023

- `[2023 NeurIPS]` **Cross-modal Active Complementary Learning with Self-refining Correspondence**  
*Yang Qin and Yuan Sun and Dezhong Peng and Joey Tianyi Zhou and Xi Peng and Peng Hu*  
[[paper]](https://openreview.net/pdf?id=UBBeUjTja8)
[[code]](https://github.com/QinYang79/CRCL)


- `[2023 TMM]` **Learning From Noisy Correspondence With Tri-Partition for Cross-Modal Matching**  
*Feng, Zerun and Zeng, Zhimin and Guo, Caili and Li, Zheng and Hu, Lin*  
[[paper]](https://ieeexplore.ieee.org/abstract/document/10258402)
 

- `[2023 CVPR]` **BiCro: Noisy Correspondence Rectification for Multi-modality Data via Bi-directional Cross-modal Similarity Consistency**  
*Shuo Yang, Zhapan XU, Kai Wang, Yang You, Hongxun Yao, Tongliang Liu, Min Xu*  
[[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Yang_BiCro_Noisy_Correspondence_Rectification_for_Multi-Modality_Data_via_Bi-Directional_Cross-Modal_CVPR_2023_paper.html)
[[code]](https://github.com/xu5zhao/BiCro)


- `[2023 CVPR]` **MSCN: Noisy Correspondence Learning with Meta Similarity Correction**  
*Han, Haochen and Miao, Kaiyao and Zheng, Qinghua and Luo, Minnan*  
[[paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Han_Noisy_Correspondence_Learning_With_Meta_Similarity_Correction_CVPR_2023_paper.html)
[[code]](https://github.com/hhc1997/MSCN)

#### 2022

- `[2022 ACMMM]` **Deep Evidential Learning with Noisy Correspondence for Cross-Modal Retrieval**  
*Qin, Yang and Peng, Dezhong and Peng, Xi and Wang, Xu and Hu, Peng*  
[[paper]](https://dl.acm.org/doi/abs/10.1145/3503161.3547922)
[[code]](https://github.com/QinYang79/DECL)

- `[2022 CVPR]` **Robust Cross-Modal Representation Learning with Progressive Self-Distillation** 
*Andonian, Alex and Chen, Shixing and Hamid, Raffay*  
[[paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Andonian_Robust_Cross-Modal_Representation_Learning_With_Progressive_Self-Distillation_CVPR_2022_paper.pdf)

- `[2022 ICML]` **Blip: Bootstrapping Language-image Pre-training for Unified Vision-language Understanding and Generation**  
Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven*  
[[paper]](https://proceedings.mlr.press/v162/li22n/li22n.pdf)
[[code]](https://github.com/salesforce/BLIP)

#### 2021

- `[2021 NeurIPS Spotlight]` **Align before Fuse: Vision and Language Representation Learning with Momentum Distillation**  
*Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong*  
[[paper]](https://proceedings.neurips.cc/paper_files/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf)
[[code]](https://github.com/salesforce/ALBEF)

- `[2021 NeurIPS Oral]` **Learning with Noisy Correspondence for Cross-modal Matching**  
*Huang, Zhenyu and Niu, Guocheng and Liu, Xiao and Ding, Wenbiao and Xiao, Xinyan and Wu, Hua and Peng, Xi*  
[[paper]](https://proceedings.neurips.cc/paper/2021/file/f5e62af885293cf4d511ceef31e61c80-Paper.pdf)
[[code]](https://github.com/XLearning-SCU/2021-NeurIPS-NCR)

## Re-identification

- `[2024 CVPR]` **Noisy-Correspondence Learning for Text-to-Image Person Re-identification**  
*Qin, Yang and Chen, Yingke and Peng, Dezhong and Peng, Xi and Zhou, Joey Tianyi and Hu, Peng*  
[[paper]](https://arxiv.org/abs/2308.09911)
[[code]](https://github.com/QinYang79/RDE)


- `[2024 IJCV]` **Robust Object Re-identification with Coupled Noisy Labels**  
*Mouxing Yang, Zhenyu Huang, Xi Peng*  
[[paper]](https://pengxi.me/wp-content/uploads/2024/01/Manuscript.pdf)
[[code]](https://github.com/XLearning-SCU/2024-IJCV-LCNL)


- `[2022 CVPR]` **Learning With Twin Noisy Labels for Visible-Infrared Person Re-Identification**  
*Yang, Mouxing and Huang, Zhenyu and Hu, Peng and Li, Taihao and Lv, Jiancheng and Peng, Xi*  
[[paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Learning_With_Twin_Noisy_Labels_for_Visible-Infrared_Person_Re-Identification_CVPR_2022_paper.pdf)
[[code]](https://github.com/XLearning-SCU/2022-CVPR-DART)


 
## Video-Text Learning


- `[2024 ICLR Oral]` **Multi-granularity Correspondence Learning from Long-term Noisy Videos**  
*Yijie Lin, Jie Zhang, Zhenyu Huang, Jia Liu, Zujie Wen, Xi Peng*  
[[paper]](https://lin-yijie.github.io/projects/Norton/)
[[code]](https://github.com/XLearning-SCU/2024-ICLR-Norton)

- `[2024 Arxiv]` **A Strong Baseline for Temporal Video-Text Alignment**  
*Li, Zeqian and Chen, Qirui and Han, Tengda and Zhang, Ya and Wang, Yanfeng and Xie, Weidi*  
[[paper]](https://arxiv.org/pdf/2312.14055.pdf)

- `[2023 TMM]` **Learning From Noisy Correspondence With Tri-Partition for Cross-Modal Matching**  
*Feng, Zerun and Zeng, Zhimin and Guo, Caili and Li, Zheng and Hu, Lin*  
[[paper]](https://ieeexplore.ieee.org/abstract/document/10258402)

- `[2023 TMM]` **Robust Video-Text Retrieval Via Noisy Pair Calibration**  
*Zhang, Huaiwen and Yang, Yang and Qi, Fan and Qian, Shengsheng and Xu, Changsheng*  
[[paper]](https://ieeexplore.ieee.org/abstract/document/10024790) 

- `[2022 CVPR Oral]` **Temporal Alignment Networks for Long-term Video**  
*Han, Tengda and Xie, Weidi and Zisserman, Andrew*  
[[paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Temporal_Alignment_Networks_for_Long-Term_Video_CVPR_2022_paper.pdf)
[[code]](https://www.robots.ox.ac.uk/~vgg/research/tan/)

- `[2021 EMNLP]` **Videoclip: Contrastive Pre-training for Zero-shot Video-text Understanding**
*Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Okhonko, Dmytro and Aghajanyan, Armen and Metze, Florian and Zettlemoyer, Luke and Feichtenhofer, Christoph*  
[[paper]](https://arxiv.org/pdf/2109.14084.pdf)
[[code]](https://github.com/facebookresearch/fairseq/tree/main/examples/MMPT)


## Image Contrastive Learning

- `[2022 CVPR]` **Robust contrastive learning against noisy views**  
*Chuang, Ching-Yao and Hjelm, R Devon and Wang, Xin and Vineet, Vibhav and Joshi, Neel and Torralba, Antonio and Jegelka, Stefanie and Song, Yale*  
[[paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Chuang_Robust_Contrastive_Learning_Against_Noisy_Views_CVPR_2022_paper.pdf)
[[code]](https://github.com/chingyaoc/RINCE)

## Graph Matching 

- `[2023 ICCV]` **Graph Matching with Noisy Correspondence**  
*Lin, Yijie and Yang, Mouxing and Yu, Jun and Hu, Peng and Zhang, Changqing and Peng, Xi*  
[[paper]](https://arxiv.org/pdf/2212.04085.pdf)
[[code]](https://github.com/Lin-Yijie/Graph-Matching-Networks)

 ## Machine Reading Comprehension

- `[2023 AAAI]` **Robust domain adaptation for machine reading comprehension**  
*Jiang, Liang and Huang, Zhenyu and Liu, Jia and Wen, Zujie and Peng, Xi*  
[[paper]](https://ojs.aaai.org/index.php/AAAI/article/view/25974)


## Multi-View Clustering 

- `[2022 TPAMI]` **Robust Multi-View Clustering With Incomplete Information**  
*Yang, Mouxing and Li, Yunfan and Hu, Peng and Bai, Jinfeng and Lv, Jian Cheng and Peng, Xi*  
[[paper]](https://ieeexplore.ieee.org/abstract/document/9723577)
[[code]](https://github.com/XLearning-SCU/2022-TPAMI-SURE)


# False Negative

- `[2024 ICLR Oral]` **Multi-granularity Correspondence Learning from Long-term Noisy Videos**  
*Yijie Lin, Jie Zhang, Zhenyu Huang, Jia Liu, Zujie Wen, Xi Peng*  
[[paper]](https://lin-yijie.github.io/projects/Norton/)
[[code]](https://github.com/XLearning-SCU/2024-ICLR-Norton)

- `[2021 ICCV]` **Crossclr: Cross-modal Contrastive Learning for Multi-modal Video Representations**  
  *Zolfaghari, Mohammadreza and Zhu, Yi and Gehler, Peter and Brox, Thomas*  
[[paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Zolfaghari_CrossCLR_Cross-Modal_Contrastive_Learning_for_Multi-Modal_Video_Representations_ICCV_2021_paper.pdf)

- `[2021 CVPR]` **Partially View-aligned Representation Learning with Noise-robust Contrastive Loss**   
*Yang, Mouxing and Li, Yunfan and Huang, Zhenyu and Liu, Zitao and Hu, Peng and Peng, Xi*  
[[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Partially_View-Aligned_Representation_Learning_With_Noise-Robust_Contrastive_Loss_CVPR_2021_paper.pdf)
[[code]](https://github.com/XLearning-SCU/2021-CVPR-MvCLN)

 
